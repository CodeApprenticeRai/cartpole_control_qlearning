{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.03998628, -0.04274393, -0.00119471,  0.00502334], dtype=float32),\n",
       " {})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Step 1: recieve an observation\n",
    "import gym\n",
    "\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "PARAM_episode_max_length = 100\n",
    "\n",
    "observation = env.reset()\n",
    "\n",
    "# for t in range(PARAM_episode_max_length):\n",
    "\n",
    "observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 2: run an episode of environment\n",
    "import gym\n",
    "\n",
    "\n",
    "env = gym.make('CartPole-v1', render_mode='human')\n",
    "\n",
    "PARAM_episode_max_length = 100\n",
    "\n",
    "observation = env.reset()\n",
    "\n",
    "for t in range(PARAM_episode_max_length):\n",
    "    random_action = env.action_space.sample()\n",
    "    observation, reward, is_terminated, is_truncated, step_info = env.step(random_action)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4,)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 3: Define, Implement the state space of the Q Algorithm\n",
    "\n",
    "#The state space is a superset of the observation spae\n",
    "\n",
    "#observations consists of 4 continuous components\n",
    "\n",
    "#There is no way I can think of to implement a continuous state space\n",
    "# without discretizing it, and thus states within the state space \n",
    "# of the  Q-learning algorithm will be represented by ranges.\n",
    "import gym, math\n",
    "\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "# Run 1000 episodes to determine observed boundary values for each component \n",
    "PARAM_number_of_episodes = 1000\n",
    "PARAM_episode_max_length = 1000\n",
    "\n",
    "state_space_num_components = env.observation_space.shape[0]\n",
    "\n",
    "class state_space_representation:\n",
    "    def __init__(self, num_components):\n",
    "        self.num_components = num_components\n",
    "        self.min_max_bounds = [[float('inf'), float('-inf')] for i in range(self.num_components)]\n",
    "        self.repr_created = False\n",
    "\n",
    "    def update(self, observation):\n",
    "        #dont update if repr_created\n",
    "        assert not (self.repr_created == True)\n",
    "\n",
    "        for i in range(self.num_components):\n",
    "            if observation[i] < self.min_max_bounds[i][0]:\n",
    "                self.min_max_bounds[i][0] = observation[i]\n",
    "            if observation[i] > self.min_max_bounds[i][1]:\n",
    "                self.min_max_bounds[i][1] = observation[i]\n",
    "        return\n",
    "\n",
    "    def create_state_space_repr(self):\n",
    "        #The state space is represented as a partition\n",
    "        # of interval (-inf, inf) into a number of \n",
    "        # subintervals.\n",
    "        # The number of intervals, and the size of \n",
    "        # each interval is determined by the length of the\n",
    "        # difference of the most extreme observed values of\n",
    "        # each component of the observation space.\n",
    "        self.state_space_component_partitions  = []\n",
    "\n",
    "        #The larger this number, the more elements will exist\n",
    "        # within the partition of each state space component.\n",
    "        self.state_space_component_complexitity_multiplier = 4\n",
    "        self.ssccm = self.state_space_component_complexitity_multiplier\n",
    "\n",
    "        for i in range(self.num_components):\n",
    "            #Given that this code was written for the cartpole environment,\n",
    "            # the intervals of each component of the cartpole observation space \n",
    "            # are all larger than 1.\n",
    "            # Thus the number of elements in each partition will always al least\n",
    "            # at least the ssccm. \n",
    "            number_of_elements_in_partition = math.ceil(self.interval_lengths[i] * self.ssccm)\n",
    "            low, high = self.min_max_bounds[i]\n",
    "\n",
    "            partition = [(-1 * float('inf'), low) ]\n",
    "            \n",
    "            interval_step_size = self.interval_lengths[i] / number_of_elements_in_partition \n",
    "\n",
    "            for j in range(number_of_elements_in_partition):\n",
    "                interval_low = low + (j * interval_step_size)\n",
    "                interval_high = low + ((j + 1) * interval_step_size)\n",
    "                interval = (interval_low, interval_high)\n",
    "                partition.append(interval)\n",
    "\n",
    "            #Should be true, but a guarantee was not proven  \n",
    "            assert (len(partition) > 0)\n",
    "            \n",
    "            print((partition[-1][1], high, partition[-1][1] - high))\n",
    "\n",
    "            partition.append((partition[-1][1], float('inf')))\n",
    "            self.state_space_component_partitions.append(partition)\n",
    "        return\n",
    "\n",
    "    def finalize(self):\n",
    "        self.repr_created = True\n",
    "        self.interval_lengths = [self.min_max_bounds[i][1] - self.min_max_bounds[i][0] for i in range(self.num_components)]\n",
    "        self.create_state_space_repr()\n",
    "        return \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tareg\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gym\\envs\\classic_control\\cartpole.py:187: UserWarning: \u001b[33mWARN: You are calling 'step()' even though this environment has already returned terminated = True. You should always call 'reset()' once you receive 'terminated = True' -- any further steps are undefined behavior.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2.490264892578125, 2.4902651, -2.384185791015625e-07)\n",
      "(5.126671314239502, 5.1266713, 0.0)\n",
      "(127.28125, 127.281265, -1.52587890625e-05)\n",
      "(19.483659744262695, 19.48366, 0.0)\n"
     ]
    }
   ],
   "source": [
    "import gym, math\n",
    "\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "# Run 1000 episodes to determine observed boundary values for each component \n",
    "PARAM_number_of_episodes = 1000\n",
    "PARAM_episode_max_length = 1000\n",
    "\n",
    "state_space_num_components = env.observation_space.shape[0]\n",
    "\n",
    "state_space_repr = state_space_representation(state_space_num_components)\n",
    "\n",
    "for episode in range(PARAM_number_of_episodes):\n",
    "    observation, _ = env.reset()\n",
    "    state_space_repr.update(observation)\n",
    "\n",
    "    for t in range(PARAM_episode_max_length):\n",
    "        random_action = env.action_space.sample()\n",
    "        observation, reward, terminate_episode_signal, _, step_info = env.step(random_action)\n",
    "        state_space_repr.update(observation)\n",
    "        \n",
    "        if terminate_episode_signal:\n",
    "            break\n",
    "\n",
    "\n",
    "state_space_repr.finalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[-2.4893837, 2.4896784],\n",
       " [-5.4872537, 5.069546],\n",
       " [-100.3937, 116.41859],\n",
       " [-17.804838, 19.29232]]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Step 3: Define, Implement the state space of the Q Algorithm\n",
    "\n",
    "#The state space is a superset of the observation spae\n",
    "\n",
    "#observations consists of 4 continuous components\n",
    "\n",
    "#There is no way I can think of to implement a continuous state space\n",
    "# without discretizing it, and thus states within the state space \n",
    "# of the  Q-learning algorithm will be represented by ranges.\n",
    "import gym\n",
    "\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "# Run 1000 episodes to determine observed boundary values for each component \n",
    "PARAM_number_of_episodes = 1000\n",
    "PARAM_episode_max_length = 1000\n",
    "\n",
    "state_space_num_components = env.observation_space.shape[0]\n",
    "\n",
    "#(min, max) observed for each component \n",
    "state_space_repr_obj = [[float('inf'), float('-inf')] for i in range(state_space_num_components)]\n",
    "\n",
    "\n",
    "\n",
    "episode_length_history = []\n",
    "\n",
    "for episode in range(PARAM_number_of_episodes):\n",
    "    observation, _ = env.reset()\n",
    "    \n",
    "    for i in range(state_space_num_components):\n",
    "        if observation[i] < state_space_repr_obj[i][0]:\n",
    "            state_space_repr_obj[i][0] = observation[i]\n",
    "        if observation[i] > state_space_repr_obj[i][1]:\n",
    "            state_space_repr_obj[i][1] = observation[i]\n",
    "\n",
    "\n",
    "    terminated_at_counter = 0 \n",
    "\n",
    "    for t in range(PARAM_episode_max_length):\n",
    "        random_action = env.action_space.sample()\n",
    "        observation, reward, terminate_episode_signal, _, step_info = env.step(random_action)\n",
    "\n",
    "        for i in range(state_space_num_components):\n",
    "            if observation[i] < state_space_repr_obj[i][0]:\n",
    "                state_space_repr_obj[i][0] = observation[i]\n",
    "            if observation[i] > state_space_repr_obj[i][1]:\n",
    "                state_space_repr_obj[i][1] = observation[i]\n",
    "\n",
    "        if not (terminate_episode_signal):\n",
    "            terminated_at_counter += 1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    episode_length_history.append(terminated_at_counter)\n",
    "\n",
    "# len(observations)\n",
    "\n",
    "# observed_bounds = []\n",
    "state_space_repr_obj\n",
    "# for i in range(state_space_num_components):\n",
    "#     observed_min = min(observations, key=lambda x: x[i])[i]\n",
    "#     observed_max = max(observations, key=lambda x: x[i])[i]\n",
    "#     observed_bounds.append((observed_min, observed_max))\n",
    "\n",
    "# observed_bounds, sum(episode_length_history)/len(episode_length_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4.96159, 9.648396, 252.36862, 38.21286]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interval_lengths = [(high - low) for low, high in observed_bounds]\n",
    "interval_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2.481074810028076, 2.4810748, 0.0)\n",
      "(4.868475914001465, 4.8684754, 4.76837158203125e-07)\n",
      "(112.92225646972656, 112.92225, 7.62939453125e-06)\n",
      "(18.531221389770508, 18.531221, 0.0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(-inf, -2.480515),\n",
       " (-2.4805150032043457, -2.2324355125427244),\n",
       " (-2.2324355125427244, -1.9843560218811036),\n",
       " (-1.9843560218811036, -1.7362765312194823),\n",
       " (-1.7362765312194823, -1.4881970405578613),\n",
       " (-1.4881970405578613, -1.2401175498962402),\n",
       " (-1.2401175498962402, -0.992038059234619),\n",
       " (-0.992038059234619, -0.7439585685729979),\n",
       " (-0.7439585685729979, -0.49587907791137686),\n",
       " (-0.49587907791137686, -0.2477995872497556),\n",
       " (-0.2477995872497556, 0.0002799034118652344),\n",
       " (0.0002799034118652344, 0.2483593940734865),\n",
       " (0.2483593940734865, 0.4964388847351078),\n",
       " (0.4964388847351078, 0.7445183753967286),\n",
       " (0.7445183753967286, 0.9925978660583499),\n",
       " (0.9925978660583499, 1.2406773567199707),\n",
       " (1.2406773567199707, 1.488756847381592),\n",
       " (1.488756847381592, 1.7368363380432132),\n",
       " (1.7368363380432132, 1.9849158287048345),\n",
       " (1.9849158287048345, 2.232995319366455),\n",
       " (2.232995319366455, 2.481074810028076),\n",
       " (2.481074810028076, inf)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Divide each component into n times the interval length\n",
    "import math\n",
    "\n",
    "state_space_component_partitions  = []\n",
    "\n",
    "#The larger this number, the more elements will exist\n",
    "# within the partition of each state space component.\n",
    "state_space_component_complexitity_multiplier = 4\n",
    "ssccm = state_space_component_complexitity_multiplier\n",
    "\n",
    "for i in range(len(interval_lengths)):\n",
    "    #Given that this code was written for the cartpole environment,\n",
    "    #the intervals of each component of the cartpole obs. space are all larger\n",
    "    # than 1.\n",
    "    # Thus the number of elements in each partition will always al least\n",
    "    # at least the ssccm. \n",
    "    number_of_elements_in_partition = math.ceil(interval_lengths[i] * ssccm)\n",
    "    low, high = observed_bounds[i]\n",
    "\n",
    "    partition = [(-1 * float('inf'), low) ]\n",
    "    \n",
    "    interval_step_size = interval_lengths[i] / number_of_elements_in_partition \n",
    "\n",
    "    for j in range(number_of_elements_in_partition):\n",
    "        interval_low = low + (j * interval_step_size)\n",
    "        interval_high = low + ((j + 1) * interval_step_size)\n",
    "        interval = (interval_low, interval_high)\n",
    "        partition.append(interval)\n",
    "\n",
    "    #Should be true, but a guarantee was not proven  \n",
    "    assert (len(partition) > 0)\n",
    "    \n",
    "    print((partition[-1][1], high, partition[-1][1] - high))\n",
    "\n",
    "    partition.append((partition[-1][1], float('inf')))\n",
    "    state_space_component_partitions.append(partition)\n",
    "\n",
    "state_space_component_partitions[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[22, 41, 1012, 155]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(partition) for partition in state_space_component_partitions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[22, 44, 1255, 158]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(partition) for partition in state_space_repr.state_space_component_partitions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = 0\n",
    "for i in range(144400000):\n",
    "    g += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class Q:\n",
    "    def __init__(self, state_space_repr, action_space_repr, learning_rate, discount_factor, epsilon, q_init_value):\n",
    "        self.state_space_repr = state_space_repr\n",
    "        self.action_space_repr = action_space_repr\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.epsilon = epsilon\n",
    "        self.q_init_value = q_init_value\n",
    "        self.q_table = {}\n",
    "        self.state_lookup_misses = 0\n",
    "        # self.delta_history = []\n",
    "        return\n",
    "\n",
    "    def map_observation_to_repr(self, observation):\n",
    "        #Given an observation, return the state space representation\n",
    "        # that it belongs to.\n",
    "        assert (len(observation) == self.state_space_repr.num_components)\n",
    "        repr = []\n",
    "\n",
    "        for i in range(self.state_space_repr.num_components):\n",
    "            for j in range(len(self.state_space_repr.state_space_component_partitions[i])):\n",
    "                low, high = self.state_space_repr.state_space_component_partitions[i][j]\n",
    "                if observation[i] >= low and observation[i] < high:\n",
    "                    repr.append(j)\n",
    "                    break\n",
    "        repr = tuple(repr)\n",
    "\n",
    "\n",
    "        return repr\n",
    "\n",
    "    def best_action(self, observation):\n",
    "        repr = self.map_observation_to_repr(observation)\n",
    "        if repr not in self.q_table:\n",
    "            self.state_lookup_misses += 1\n",
    "            #place current state in table\n",
    "            self.q_table[repr] = {i:self.q_init_value for i in range(self.action_space_repr.n)}\n",
    "        \n",
    "        if random.random() <= self.epsilon:\n",
    "            return self.action_space_repr.sample()\n",
    "\n",
    "        all_action_values_equal = True\n",
    "        best_action = 0\n",
    "        best_value = self.q_init_value\n",
    "        \n",
    "        for action in self.q_table[repr]:\n",
    "            value = self.q_table[repr][action] \n",
    "            if (value > best_value):\n",
    "                all_action_values_equal = False\n",
    "                best_action = action\n",
    "                best_value = value\n",
    "\n",
    "        if all_action_values_equal:\n",
    "            return self.action_space_repr.sample()\n",
    "        else:\n",
    "            return best_action\n",
    "    \n",
    "    def update(self, observation, action, reward, next_observation):\n",
    "        repr = self.map_observation_to_repr(observation)\n",
    "        next_repr = self.map_observation_to_repr(next_observation)\n",
    "\n",
    "        if repr not in self.q_table:\n",
    "            #place current state in table\n",
    "            self.state_lookup_misses += 1\n",
    "            self.q_table[repr] = {i:self.q_init_value for i in range(self.action_space_repr.n)}\n",
    "\n",
    "        if next_repr not in self.q_table:\n",
    "            #place next state in table\n",
    "            self.state_lookup_misses += 1\n",
    "            self.q_table[next_repr] = {i:self.q_init_value for i in range(self.action_space_repr.n)}\n",
    "\n",
    "        \n",
    "        #update q value\n",
    "        old_value = self.q_table[repr][action]\n",
    "        current_value_information = (1 - self.learning_rate) * self.q_table[repr][action]\n",
    "        temporal_diff_target = self.learning_rate * (reward + self.discount_factor * max(self.q_table[next_repr].values()) )\n",
    "        self.q_table[repr][action] = current_value_information + temporal_diff_target  \n",
    "\n",
    "        # delta = self.q_table[repr][action] - old_value\n",
    "        # print( (old_value, self.q_table[repr][action], delta, temporal_diff_target), sep=\"\", end=\" \" )\n",
    "        # self.delta_history.append(delta)\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_reward_per_session = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Run Algorithm, Achieve Convergence\n",
    "# Code should take in an (observation, action), determine the \n",
    "# bounding interval for the observation.\n",
    "# If a state has not been accessed before, it's \n",
    "# pre-updated Q value is 0.\n",
    "env = gym.make('CartPole-v1', render_mode='human')\n",
    "\n",
    "# Run 1000 episodes to determine observed boundary values for each component \n",
    "PARAM_number_of_episodes = 30\n",
    "PARAM_episode_max_length = 1000\n",
    "\n",
    "PARAM_learning_rate = 0.8\n",
    "PARAM_discount_factor = 0.8\n",
    "\n",
    "table = q.q_table\n",
    "\n",
    "q = Q(\n",
    "    state_space_repr=state_space_repr, \n",
    "    action_space_repr=env.action_space, \n",
    "    learning_rate=PARAM_learning_rate, \n",
    "    discount_factor=PARAM_discount_factor,\n",
    "    epsilon=0.2,\n",
    "    q_init_value=0\n",
    ")\n",
    "\n",
    "q.q_table = table\n",
    "\n",
    "import pickle \n",
    "\n",
    "with open('q-table.pkl', 'wb') as f:\n",
    "    pickle.dump(q.q_table, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Run Algorithm, Achieve Convergence\n",
    "# Code should take in an (observation, action), determine the \n",
    "# bounding interval for the observation.\n",
    "# If a state has not been accessed before, it's \n",
    "# pre-updated Q value is 0.\n",
    "import gym\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "PARAM_number_of_episodes = 100\n",
    "PARAM_episode_max_length = 1000\n",
    "\n",
    "PARAM_learning_rate = 0.8\n",
    "PARAM_discount_factor = 0.8\n",
    "\n",
    "table = q.q_table\n",
    "\n",
    "q = Q(\n",
    "    state_space_repr=state_space_repr, \n",
    "    action_space_repr=env.action_space, \n",
    "    learning_rate=PARAM_learning_rate, \n",
    "    discount_factor=PARAM_discount_factor,\n",
    "    epsilon=0.2,\n",
    "    q_init_value=0\n",
    ")\n",
    "\n",
    "q.q_table = table\n",
    "\n",
    "import pickle \n",
    "\n",
    "with open('q-table.pkl', 'wb') as f:\n",
    "    pickle.dump(q.q_table, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29, 240614, 19, 54)\n",
      "(28, 240677, 63, 22)\n",
      "(27, 240726, 49, 9)\n",
      "(26, 240864, 138, 42)\n",
      "(25, 240953, 89, 37)\n",
      "(24, 241380, 427, 44)\n",
      "(23, 241559, 179, 37)\n",
      "(22, 241784, 225, 12)\n",
      "(21, 241806, 22, 35)\n",
      "(20, 241865, 59, 31)\n",
      "(19, 241922, 57, 32)\n",
      "(18, 242088, 166, 24)\n",
      "(17, 242122, 34, 29)\n",
      "(16, 242286, 164, 54)\n",
      "(15, 242338, 52, 46)\n",
      "(14, 242437, 99, 20)\n",
      "(13, 242542, 105, 24)\n",
      "(12, 242607, 65, 26)\n",
      "(11, 242658, 51, 7)\n",
      "(10, 242737, 79, 19)\n",
      "(9, 242774, 37, 13)\n",
      "(8, 242822, 48, 27)\n",
      "(7, 242861, 39, 33)\n",
      "(6, 242893, 32, 27)\n",
      "(5, 242968, 75, 15)\n",
      "(4, 243002, 34, 71)\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "reward_history = []\n",
    "\n",
    "for episode in range(PARAM_number_of_episodes):\n",
    "    observation, _ = env.reset()\n",
    "\n",
    "    episode_reward = 0\n",
    "    misses_at_start = q.state_lookup_misses\n",
    "    for t in range(PARAM_episode_max_length):\n",
    "        env.render()\n",
    "        action = q.best_action(observation)\n",
    "        next_observation, reward, terminate_episode_signal, _, step_info = env.step(action)\n",
    "        q.update(observation, action, reward, next_observation)\n",
    "        \n",
    "        episode_reward += reward\n",
    "        observation = next_observation\n",
    "\n",
    "        if terminate_episode_signal:\n",
    "            break\n",
    "    \n",
    "    reward_history.append(episode_reward)\n",
    "\n",
    "    misses_during_episode = q.state_lookup_misses - misses_at_start\n",
    "    episodes_left = PARAM_number_of_episodes - episode - 1\n",
    "    exp_info = (episodes_left, len(q.q_table), misses_during_episode, episode_reward) \n",
    "    print(exp_info)\n",
    "\n",
    "env.close()\n",
    "\n",
    "import numpy as np\n",
    "import win32api\n",
    "\n",
    "session_reward = np.mean(reward_history)\n",
    "mean_reward_per_session.append(session_reward)\n",
    "\n",
    "\n",
    "win32api.MessageBox(0, 'Done', 'DoneMessage', 0x00001000) \n",
    "\n",
    "mean_reward_per_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9.885714285714286,\n",
       " 23.9,\n",
       " 20.3,\n",
       " 12.1,\n",
       " 9.9,\n",
       " 17.3,\n",
       " 12.2,\n",
       " 13.86,\n",
       " 18.6,\n",
       " 13.066666666666666,\n",
       " 11.99,\n",
       " 18.67,\n",
       " 35.5,\n",
       " 28.82,\n",
       " 30.29]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_reward_per_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "with open('q-table.pkl', 'rb') as f:\n",
    "    table = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
